{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 14:49:37.067847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 14:49:37.067877: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 14:49:37.069116: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 14:49:37.074320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 14:49:37.598595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# from save_result import save_data\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load MNIST DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, res):\n",
    "    state = res[\"state\"]\n",
    "    size = res[\"size\"]\n",
    "    device = res[\"device\"]\n",
    "    inference_time = res[\"time\"]\n",
    "    accuracy = res['accuracy']\n",
    "\n",
    "    with open(path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([state, size, device, inference_time, accuracy])\n",
    "    \n",
    "    print('Data appended successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)  # Add channel dimension (28x28x1)\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 14:51:04.443453: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.512802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.513018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.515005: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.515160: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.515302: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.574936: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.575106: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.575252: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 14:51:04.575367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19819 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# model = models.Sequential([\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "    # Increase the number of filters in the Conv2D layers\n",
    "model.add(Conv2D(160, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))  # 5x 32 filters = 160 filters\n",
    "model.add(Conv2D(320, (3, 3), activation='relu'))  # 5x 64 filters = 320 filters\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Increase the number of units in Dense layers\n",
    "model.add(Dense(640, activation='relu'))  # 5x 128 units = 640 units\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer remains unchanged    \n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 14:51:10.021675: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-09-12 14:51:10.326659: I external/local_xla/xla/service/service.cc:168] XLA service 0x76fae1dcd490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-12 14:51:10.326680: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-09-12 14:51:10.330978: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726132870.385544 2602632 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0941 - accuracy: 0.9716 - val_loss: 0.0393 - val_accuracy: 0.9865\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.0396 - val_accuracy: 0.9866\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.0337 - val_accuracy: 0.9906\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.0399 - val_accuracy: 0.9893\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.0387 - val_accuracy: 0.9900\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0507 - val_accuracy: 0.9897\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0617 - val_accuracy: 0.9888\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0644 - val_accuracy: 0.9881\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0808 - val_accuracy: 0.9890\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.0745 - val_accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x77042711fc90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./weighted_mnist.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convert the Keras model to TensorFlow Lite format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_3q8y3lh/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_3q8y3lh/assets\n",
      "2024-09-12 15:05:48.533895: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-09-12 15:05:48.533918: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-09-12 15:05:48.534045: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp_3q8y3lh\n",
      "2024-09-12 15:05:48.534588: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-09-12 15:05:48.534597: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp_3q8y3lh\n",
      "2024-09-12 15:05:48.536094: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-09-12 15:05:48.632314: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp_3q8y3lh\n",
      "2024-09-12 15:05:48.640953: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 106908 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 5, Total Ops 19, % non-converted = 26.32 %\n",
      " * 5 ARITH ops\n",
      "\n",
      "- arith.constant:    5 occurrences  (f32: 4, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (uq_8: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.load_model('weighted_mnist.keras')\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# # Optionally, enable optimizations to improve conversion and efficiency\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# # Enable conversion for TensorFlow ops if needed\n",
    "# converter.target_spec.supported_ops = [\n",
    "#     tf.lite.OpsSet.TFLITE_BUILTINS,   # Enable TFLite built-in operations\n",
    "#     tf.lite.OpsSet.SELECT_TF_OPS      # Enable TensorFlow operations (Flex ops)\n",
    "# ]\n",
    "\n",
    "# # Convert the model\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the converted model\n",
    "# with open('weighted_mnist_first.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "\n",
    "# print(\"TFLite model has been successfully converted and saved.\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable optimizations\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('weighted_mnist_second.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Type: <class 'numpy.float32'>\n",
      "Output Type: <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"weighted_mnist_second.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Print input and output types\n",
    "print(f\"Input Type: {input_details[0]['dtype']}\")\n",
    "print(f\"Output Type: {output_details[0]['dtype']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp88uxgamf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp88uxgamf/assets\n",
      "/home/server/anaconda3/lib/python3.11/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-09-12 15:09:35.739871: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-09-12 15:09:35.739901: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-09-12 15:09:35.740024: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp88uxgamf\n",
      "2024-09-12 15:09:35.740797: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-09-12 15:09:35.740807: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp88uxgamf\n",
      "2024-09-12 15:09:35.742406: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-09-12 15:09:35.838574: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp88uxgamf\n",
      "2024-09-12 15:09:35.845755: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 105729 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 9, Total Ops 19, % non-converted = 47.37 %\n",
      " * 9 ARITH ops\n",
      "\n",
      "- arith.constant:    9 occurrences  (f32: 8, i32: 1)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to integer quantized format\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "def representative_dataset_gen():\n",
    "    for i in range(100):\n",
    "        image = x_train[i].reshape(1, 28, 28, 1)\n",
    "        yield [image]\n",
    "\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized TFLite model\n",
    "with open('weighted_mnist_quant.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **For TPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from pycoral.utils.edgetpu import make_interpreter\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the compiled Edge TPU model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# interpreter = make_interpreter(\"mnist_model_quant_edgetpu.tflite\")\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlite\u001b[38;5;241m.\u001b[39mInterpreter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_model_quant_edgetpu.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mallocate_tensors()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load and preprocess an image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# for TPU\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycoral.adapters import common\n",
    "from pycoral.utils.edgetpu import make_interpreter\n",
    "\n",
    "# Load the compiled Edge TPU model\n",
    "interpreter = make_interpreter(\"mnist_model_quant_edgetpu.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Load and preprocess an image\n",
    "image = Image.open(\"test_image.png\").convert('L').resize((28, 28))\n",
    "input_data = np.asarray(image, dtype=np.uint8).reshape(1, 28, 28, 1)\n",
    "\n",
    "# Set input tensor and invoke\n",
    "common.set_input(interpreter, input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output\n",
    "output = common.output_tensor(interpreter, 0)\n",
    "print(\"Predicted digit:\", np.argmax(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **For CPU (Single Image)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time on CPU: 0.006709 seconds\n",
      "Predicted Digit: 8\n"
     ]
    }
   ],
   "source": [
    "# Make Interpreter\n",
    "# For Single Image \n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "# Load the quantized TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"weighted_mnist_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare a test input image (e.g., a random image or from your dataset)\n",
    "# Here we're using a random image. You can replace this with actual data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.random.randint(0, 255, size=input_shape, dtype=np.uint8)\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# End time after inference\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the output\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "predicted_digit = np.argmax(output_data)\n",
    "\n",
    "# Print the inference time\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "res = {\n",
    "    \"state\" : \"Without Quantization\",\n",
    "    \"device\" : 'CPU',\n",
    "    \"size\" : 'Single Image',\n",
    "    \"time\" : inference_time,\n",
    "    \"accuracy\" : 0\n",
    "}\n",
    "# save_data(\"./results/result.csv\", res)\n",
    "print(f\"Inference Time on CPU: {inference_time:.6f} seconds\")\n",
    "print(f\"Predicted Digit: {predicted_digit}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CPU (Test Data)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:49<00:00, 203.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': 'With Quantization', 'size': 'Test Image', 'device': 'CPU', 'time': 0.004878571486473083, 'accuracy': 0.989}\n",
      "Average Inference Time on CPU: 0.004879 seconds\n",
      "Accuracy on Test Dataset: 98.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# For Test Data Set\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the MNIST dataset (you can use the same data preprocessed for model training)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the test data (reshape and normalize to uint8 as expected by the quantized model)\n",
    "x_test = x_test.astype('uint8')  # Convert to uint8 (as required by the quantized model)\n",
    "x_test = np.expand_dims(x_test, -1)  # Add the channel dimension (28x28x1)\n",
    "\n",
    "# Load the quantized TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"weighted_mnist_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to perform inference and measure time for a single image\n",
    "def run_inference(interpreter, input_data):\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    start_time = time.time()\n",
    "    interpreter.invoke()\n",
    "    end_time = time.time()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return np.argmax(output_data), (end_time - start_time)\n",
    "\n",
    "# Run inference on the test dataset and calculate average inference time\n",
    "total_time = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    input_data = np.expand_dims(x_test[i], axis=0)  # Reshape to match the model's input shape (1, 28, 28, 1)\n",
    "    predicted_digit, inference_time = run_inference(interpreter, input_data)\n",
    "    \n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if predicted_digit == y_test[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate average inference time\n",
    "avg_inference_time = total_time / len(x_test)\n",
    "accuracy = correct_predictions / len(x_test)\n",
    "\n",
    "res = {\n",
    "    \"state\" : \"With Quantization\",\n",
    "    \"size\" : 'Test Image',\n",
    "    \"device\" : 'CPU',\n",
    "    \"time\" : avg_inference_time,\n",
    "    \"accuracy\" : accuracy\n",
    "}\n",
    "print(res)\n",
    "# save_data(\"./results/result.csv\", res)\n",
    "print(f\"Average Inference Time on CPU: {avg_inference_time:.6f} seconds\")\n",
    "print(f\"Accuracy on Test Dataset: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# final\n",
    "# 100%|██████████| 10000/10000 [00:49<00:00, 203.41it/s]{'state': 'With Quantization', 'size': 'Test Image', 'device': 'CPU', 'time': 0.004878571486473083, 'accuracy': 0.989}\n",
    "# Average Inference Time on CPU: 0.004879 seconds\n",
    "# Accuracy on Test Dataset: 98.90%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **For Local Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random Image \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Loads an image from the specified path, converts it to grayscale, resizes it to 28x28,\n",
    "    and converts it into the required format (uint8, shape (1, 28, 28, 1)) for inference.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed image ready for model inference.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = Image.open(image_path).convert('L')  # Convert to grayscale (L mode)\n",
    "    \n",
    "    # Resize the image to 28x28 as required by the MNIST model\n",
    "    img = img.resize((28, 28))\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = np.array(img, dtype=np.uint8)\n",
    "    \n",
    "    # Reshape the array to (1, 28, 28, 1) as required by the model\n",
    "    img_array = np.expand_dims(img_array, axis=(0, -1))\n",
    "    \n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 201.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit: 0\n",
      "Total Inference Time: 4.933213233948 seconds\n",
      "Average Inference Time: 0.004933213234 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "preprocessed_image = load_and_preprocess_image(\"./zero.png\")\n",
    "\n",
    "# Run inference using the preprocessed image\n",
    "total_time = 0\n",
    "correct_predictions = 0\n",
    "for i in tqdm(range(1000)):\n",
    "    predicted_digit, inference_time = run_inference(interpreter, preprocessed_image)\n",
    "    total_time += inference_time\n",
    "    if predicted_digit == 0:\n",
    "        correct_predictions += 1\n",
    "\n",
    "avg_inference_time = total_time/1000\n",
    "print(f\"Predicted Digit: {predicted_digit}\")\n",
    "print(f\"Total Inference Time: {total_time:.12f} seconds\")\n",
    "print(f\"Average Inference Time: {avg_inference_time:.12f} seconds\")\n",
    "\n",
    "# 100%|██████████| 1000/1000 [00:04<00:00, 201.38it/s]\n",
    "# Predicted Digit: 0\n",
    "# Total Inference Time: 4.933213233948 seconds\n",
    "# Average Inference Time: 0.004933213234 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Without Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset (you can use the same data preprocessed for model training)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the test data (reshape and normalize as required by the model)\n",
    "x_test = x_test.astype('float32') / 255.0  # Normalize the data to [0, 1]\n",
    "x_test = np.expand_dims(x_test, -1)  # Add the channel dimension (28x28x1)\n",
    "\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(\"mnist_cnn.keras\")  # Path to your saved Keras model\n",
    "\n",
    "# Function to perform inference and measure time for a single image\n",
    "def run_inference(model, input_data):\n",
    "    start_time = time.time()\n",
    "    prediction = model.predict(input_data)  # Get prediction\n",
    "    end_time = time.time()\n",
    "    \n",
    "    predicted_digit = np.argmax(prediction, axis=-1)  # Get the class with the highest probability\n",
    "    return predicted_digit[0], (end_time - start_time)\n",
    "\n",
    "# Run inference on the test dataset and calculate average inference time\n",
    "total_time = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    input_data = np.expand_dims(x_test[i], axis=0)  # Reshape to match the model's input shape (1, 28, 28, 1)\n",
    "    predicted_digit, inference_time = run_inference(model, input_data)\n",
    "    \n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if predicted_digit == y_test[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate average inference time\n",
    "avg_inference_time = total_time / len(x_test)\n",
    "accuracy = correct_predictions / len(x_test)\n",
    "\n",
    "res = {\n",
    "    \"state\" : \"Without Quantization\",\n",
    "    \"size\" : \"test_data\",\n",
    "    \"device\" : 'CPU',\n",
    "    \"time\" : avg_inference_time,\n",
    "    \"accuracy\" : accuracy\n",
    "}\n",
    "\n",
    "save_data(\"./results/result.csv\", res)\n",
    "\n",
    "print(f\"Average Inference Time on CPU: {avg_inference_time:.6f} seconds\")\n",
    "print(f\"Accuracy on Test Dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Inference Time on CPU: {avg_inference_time:.6f} seconds\")\n",
    "print(f\"Accuracy on Test Dataset: {accuracy * 100:.2f}%\")\n",
    "# Average Inference Time on CPU: 0.081543 seconds\n",
    "# Accuracy on Test Dataset: 98.81%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ON GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable GPU memory growth (optional, but useful for avoiding full memory allocation)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs detected: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Load the MNIST dataset (you can use the same data preprocessed for model training)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(_, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the test data (reshape and normalize as required by the model)\n",
    "x_test = x_test.astype('float32') / 255.0  # Normalize the data to [0, 1]\n",
    "x_test = np.expand_dims(x_test, -1)  # Add the channel dimension (28x28x1)\n",
    "\n",
    "# Load the Keras model\n",
    "model = tf.keras.models.load_model(\"mnist_cnn.keras\")  # Path to your saved Keras model\n",
    "\n",
    "# Function to perform inference and measure time for a single image\n",
    "def run_inference(model, input_data):\n",
    "    start_time = time.time()\n",
    "    prediction = model.predict(input_data)  # Get prediction (uses GPU if available)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    predicted_digit = np.argmax(prediction, axis=-1)  # Get the class with the highest probability\n",
    "    return predicted_digit[0], (end_time - start_time)\n",
    "\n",
    "# Run inference on the test dataset and calculate average inference time\n",
    "total_time = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    input_data = np.expand_dims(x_test[i], axis=0)  # Reshape to match the model's input shape (1, 28, 28, 1)\n",
    "    predicted_digit, inference_time = run_inference(model, input_data)\n",
    "    \n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if predicted_digit == y_test[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate average inference time\n",
    "avg_inference_time = total_time / len(x_test)\n",
    "accuracy = correct_predictions / len(x_test)\n",
    "\n",
    "print(f\"Average Inference Time on {'GPU' if gpus else 'CPU'}: {avg_inference_time:.6f} seconds\")\n",
    "print(f\"Accuracy on Test Dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Inference Time on {'GPU' if gpus else 'CPU'}: {avg_inference_time:.6f} seconds\")\n",
    "print(f\"Accuracy on Test Dataset: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Inference Time on {'GPU' if gpus else 'CPU'}: {avg_inference_time:.6f} seconds\")\n",
    "print(f\"Accuracy on Test Dataset: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
